{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from __future__ import print_function\n",
    "from math import ceil\n",
    "import numpy as np\n",
    "import sys\n",
    "import pdb\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "import generator\n",
    "import discriminator\n",
    "import oracle\n",
    "import helpers\n",
    "from jak_helpers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA = False\n",
    "\n",
    "REAL_DATA_PATH = './JAK2New.txt'\n",
    "ORACLE_DATA_PATH = './jak.pt'\n",
    "\n",
    "#These must be initialized\n",
    "VOCAB_SIZE = None\n",
    "MAX_SEQ_LEN = None\n",
    "START_LETTER = None\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "#Commented epochs for debugging\n",
    "MLE_TRAIN_EPOCHS = 1 #100\n",
    "ADV_TRAIN_EPOCHS = 50\n",
    "POS_NEG_SAMPLES = 1000\n",
    "\n",
    "GEN_EMBEDDING_DIM = 32\n",
    "GEN_HIDDEN_DIM = 32\n",
    "DIS_EMBEDDING_DIM = 64\n",
    "DIS_HIDDEN_DIM = 64\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nLook into using the following:\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Encode the real data to int tokens, and set the key global variables from that encoding.\n",
    "VOCAB_SIZE, MAX_SEQ_LEN, START_LETTER = encode_data(REAL_DATA_PATH, ORACLE_DATA_PATH)\n",
    "\n",
    "'''\n",
    "TODO: Oracle is currently a seeded random generator. We need to turn it into something that \n",
    "generates real data in a token-wise manner.\n",
    "'''\n",
    "#oracle = generator.Generator(GEN_EMBEDDING_DIM, GEN_HIDDEN_DIM, VOCAB_SIZE, MAX_SEQ_LEN, gpu=CUDA)\n",
    "oracle_obj = oracle.Oracle(VOCAB_SIZE, MAX_SEQ_LEN, './jak.pt', )\n",
    "\n",
    "'''\n",
    "This is the 'seed' part of the oracle. We likely won't need it.\n",
    "'''\n",
    "#oracle.load_state_dict(torch.load(oracle_state_dict_path))\n",
    "\n",
    "'''\n",
    "Oracle samples should be the real data - i.e., the JAK data.\n",
    "'''\n",
    "#oracle_samples = torch.load(oracle_samples_path).type(torch.LongTensor)\n",
    "'''\n",
    "Look into using the following:\n",
    "'''\n",
    "# samples for the new oracle can be generated using helpers.batchwise_sample()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from generator_attention import Generator_attention as ga"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12428/12428 [00:03<00:00, 3264.07it/s]\n"
     ]
    }
   ],
   "source": [
    "#(VOCAB_SIZE, g_emb_dim, g_hidden_dim, g_sequence_len, BATCH_SIZE, opt.cuda, POSITIVE_FILE)\n",
    "#num_emb, emb_dim, hidden_dim, seq_len, batch_size, use_cuda, real_data_path, test_mode = False):\n",
    "g = ga(VOCAB_SIZE, GEN_EMBEDDING_DIM, GEN_HIDDEN_DIM, MAX_SEQ_LEN, BATCH_SIZE, True, ORACLE_DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(37, 32, 32, 99, 32, True, './jak.pt')\n"
     ]
    }
   ],
   "source": [
    "print((VOCAB_SIZE, GEN_EMBEDDING_DIM, GEN_HIDDEN_DIM, MAX_SEQ_LEN, BATCH_SIZE, True, ORACLE_DATA_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nSampling looks like it's very inefficient/slow. \\nWe'd want to speed that up, but first things first - plug the outputs into \\nthe discriminator.\\n\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Sampling looks like it's very inefficient/slow. \n",
    "We'd want to speed that up, but first things first - plug the outputs into \n",
    "the discriminator.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/99 [00:00<00:15,  6.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling output - Seq Len: 99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 30/99 [00:05<00:13,  5.16it/s]\n"
     ]
    }
   ],
   "source": [
    "res = g.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Premature termination due to encountering eos - end of string - token in generator.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[11,  8, 12, 27, 18, 33, 17, 12, 11, 35,  1,  2,  2,  2,  2,  2,  2,  2,\n",
       "          2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2],\n",
       "        [25,  3, 13, 34, 31, 26, 28, 32, 20,  7,  6,  7, 10, 20, 14, 13,  7, 26,\n",
       "         28, 17,  7,  5, 33, 35, 20, 16, 31, 15, 13,  7,  1],\n",
       "        [27, 10, 31, 27, 34, 29, 12, 36, 32, 25, 24, 23,  8, 36, 23,  6, 21,  1,\n",
       "          2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nTransformer sample output isn't padded, so we have to retroactively pad it out to \\nMAX_SEQ_LEN.\\n\\nPad with 25.\\n\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Transformer sample output isn't padded, so we have to retroactively pad it out to \n",
    "MAX_SEQ_LEN.\n",
    "\n",
    "Pad with 25.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[11.,  8., 12., 27., 18., 33., 17., 12., 11., 35.,  1.,  2.,  2.,\n",
       "         2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,\n",
       "         2.,  2.,  2.,  2.,  2., 25., 25., 25., 25., 25., 25., 25., 25.,\n",
       "        25., 25., 25., 25., 25., 25., 25., 25., 25., 25., 25., 25., 25.,\n",
       "        25., 25., 25., 25., 25., 25., 25., 25., 25., 25., 25., 25., 25.,\n",
       "        25., 25., 25., 25., 25., 25., 25., 25., 25., 25., 25., 25., 25.,\n",
       "        25., 25., 25., 25., 25., 25., 25., 25., 25., 25., 25., 25., 25.,\n",
       "        25., 25., 25., 25., 25., 25., 25., 25.],\n",
       "       [25.,  3., 13., 34., 31., 26., 28., 32., 20.,  7.,  6.,  7., 10.,\n",
       "        20., 14., 13.,  7., 26., 28., 17.,  7.,  5., 33., 35., 20., 16.,\n",
       "        31., 15., 13.,  7.,  1., 25., 25., 25., 25., 25., 25., 25., 25.,\n",
       "        25., 25., 25., 25., 25., 25., 25., 25., 25., 25., 25., 25., 25.,\n",
       "        25., 25., 25., 25., 25., 25., 25., 25., 25., 25., 25., 25., 25.,\n",
       "        25., 25., 25., 25., 25., 25., 25., 25., 25., 25., 25., 25., 25.,\n",
       "        25., 25., 25., 25., 25., 25., 25., 25., 25., 25., 25., 25., 25.,\n",
       "        25., 25., 25., 25., 25., 25., 25., 25.],\n",
       "       [27., 10., 31., 27., 34., 29., 12., 36., 32., 25., 24., 23.,  8.,\n",
       "        36., 23.,  6., 21.,  1.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,\n",
       "         2.,  2.,  2.,  2.,  2., 25., 25., 25., 25., 25., 25., 25., 25.,\n",
       "        25., 25., 25., 25., 25., 25., 25., 25., 25., 25., 25., 25., 25.,\n",
       "        25., 25., 25., 25., 25., 25., 25., 25., 25., 25., 25., 25., 25.,\n",
       "        25., 25., 25., 25., 25., 25., 25., 25., 25., 25., 25., 25., 25.,\n",
       "        25., 25., 25., 25., 25., 25., 25., 25., 25., 25., 25., 25., 25.,\n",
       "        25., 25., 25., 25., 25., 25., 25., 25.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padding = np.zeros((res.shape[0],MAX_SEQ_LEN-res.shape[1]))\n",
    "padding += 25\n",
    "np.concatenate([res, padding], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12428/12428 [00:04<00:00, 3095.51it/s]\n"
     ]
    }
   ],
   "source": [
    "from core.data_iter import GenDataIter\n",
    "\n",
    "data_loader = GenDataIter(ORACLE_DATA_PATH, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 99])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_loader.next()[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "Gen and Dis are both randomly initialized.\n",
    "'''\n",
    "gen = generator.Generator(GEN_EMBEDDING_DIM, GEN_HIDDEN_DIM, VOCAB_SIZE, MAX_SEQ_LEN, gpu=CUDA)\n",
    "dis = discriminator.Discriminator(DIS_EMBEDDING_DIM, DIS_HIDDEN_DIM, VOCAB_SIZE, MAX_SEQ_LEN, gpu=CUDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[10, 23,  5,  ..., 21, 28, 32],\n",
       "        [21,  4,  4,  ...,  4, 34,  8],\n",
       "        [28, 28, 11,  ..., 26, 12, 17],\n",
       "        ...,\n",
       "        [ 8,  2, 17,  ..., 31, 14, 17],\n",
       "        [34, 32,  7,  ..., 16, 26,  1],\n",
       "        [ 3, 34, 19,  ..., 12,  0, 36]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen.sample(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[22, 16, 26,  ..., 25, 25,  5],\n",
       "        [17, 30,  7,  ..., 25, 25,  5],\n",
       "        [16, 22, 30,  ..., 25, 25,  5],\n",
       "        ...,\n",
       "        [16, 16,  1,  ..., 25, 25,  5],\n",
       "        [16, 21, 30,  ..., 25, 25,  5],\n",
       "        [16, 33,  7,  ..., 25, 25,  5]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oracle_obj.sample(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = helpers.batchwise_sample(gen, POS_NEG_SAMPLES, BATCH_SIZE)\n",
    "r = helpers.batchwise_sample(oracle_obj, POS_NEG_SAMPLES, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[16,  7,  3,  6, 33, 11, 17, 32, 12, 10, 12, 17, 27, 10, 27, 21,  4, 24,\n",
       "         22, 21, 27, 23, 28, 14, 23, 22, 14, 28, 18, 17, 21,  1, 20, 23,  9, 21,\n",
       "          2, 14,  8, 15, 18,  8, 30, 19, 33, 31, 22, 13, 15,  2, 27, 31, 15, 35,\n",
       "         13, 29,  8, 11, 10, 27,  2,  3,  8, 10, 28, 36, 36, 35, 24, 12, 19, 35,\n",
       "         16, 29, 33,  8, 19,  1, 12, 21,  2, 19, 27,  0, 11,  2, 17,  4, 26,  7,\n",
       "         15, 28, 32,  3,  0, 10,  3,  5, 17]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen.sample(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/phohenecker/pytorch-transformer\n",
      "  Cloning https://github.com/phohenecker/pytorch-transformer to /tmp/pip-req-build-4kmc2gp_\n",
      "Collecting insanity>=2017.1 (from transformer==2018.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/9a/64/ab5956d8360ed58e0c2f3bcf3a4c53b511dcc22ff84cec26cff559fe811c/insanity-2017.1.tar.gz\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformer==2018.1) (1.15.4)\n",
      "Requirement already satisfied: torch>=0.4.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformer==2018.1) (1.1.0)\n",
      "Building wheels for collected packages: transformer, insanity\n",
      "  Running setup.py bdist_wheel for transformer ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /tmp/pip-ephem-wheel-cache-9jmf2zat/wheels/88/ba/b9/970f87f50d549a7fd30b03b105af0d79bf210f2dc3ead0789b\n",
      "  Running setup.py bdist_wheel for insanity ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/ec2-user/.cache/pip/wheels/83/cc/18/590868a1a960f5421ec4846bfe93b228ea5e3f54c0c129b303\n",
      "Successfully built transformer insanity\n",
      "\u001b[31mfastai 1.0.55 requires nvidia-ml-py3, which is not installed.\u001b[0m\n",
      "\u001b[31mthinc 6.12.1 has requirement msgpack<0.6.0,>=0.5.6, but you'll have msgpack 0.6.0 which is incompatible.\u001b[0m\n",
      "Installing collected packages: insanity, transformer\n",
      "Successfully installed insanity-2017.1 transformer-2018.1\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 19.3.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/phohenecker/pytorch-transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
